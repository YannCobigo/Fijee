bla

\newpage

%-------------------------------------------------------------------------------

\section{Finite element methode\label{FEM}}

The \FEM{} is a method of approximating the solution of \PED{} which is constructed from an equivalent formulation of the problem. This equivalent formulation is called {\it weak formulation}\index{weak formulation}. The general approach of the \FEM{} can be summarized in the following way: 

\begin{itemize}
\item We have a \PED{} with boundary conditions. We multiply this equation by a test function, we integrate over the entire simulation domain.
\item The second step is to make a partial integration in order to obtain the weak formulation.
\item At this stage, the domain discretization is essential (triangular or quadrilateral in two dimensions, tetrahedral and hexahedral in three dimensions). 
\end{itemize}


\newpage

%-------------------------------------------------------------------------------

\section{Mathematical background}

The method principal is to approach on a domain $\Omega$, the solution of a \PED{} with boundary conditions on the edges of $\partial\Omega$ with local fields. Local fields defined on each subdomain $\Omega_{i}$ are generally polynomial interpolation. The overall solution is the concatenation of all local fields. The quality of the solution depends on the division of the sub-domain.

\subsection{Weak formulation}

\subsection{Galerkin method}

\subsection{Introduction to FEM}

\newpage

%-------------------------------------------------------------------------------

\section{Sparse matrices}

\subsection{Sparse matrix formats}
\subsubsection{Coordinate list (COO)}
\subsubsection{Compressed sparse row (CSR or CRS)}

\newpage

%-------------------------------------------------------------------------------

\section{Krylov space}

Goal $Ax = b$\\
Say something we don't care about direct methods. We only work on iterative methods (Krylov space)\\

Development: \\
$LU$ factorisation\\


A Krylov method, named after Aleksey Nikolaevich Krylov (1863 - 1945) was a Russian naval engineer, applied mathematician, is a method that solves the system $Ax = b$ by repeatedly performing matrix-vector multiplications involving $A$. this excludes methods like biconjugate gradient method that also require matrix-vector multiplications involving the conjugate transpose $A^{*}$.\\
We will show that the solution to a nonsingular linear system $Ax = b$ lies in a Krylov space whose dimension is the degree of the minimal polynomial of $A$\footnote{The minimal polynomial of a matrix $A$ is defined by: $P(A) = \alpha_{0}I + \alpha_{1}A + \cdots \alpha_{k}A^{k} = 0$ }. 

\paragraph{Theorem}{\it If the minimal polynomial of the nonsingular matrix $A$ has degree $k$, then the solution to $Ax = b$ lies in the space $\mathcal{K}_{k}(A, b)$.}

Along the caracteristic polynomial, the minimal polynomial provide the eigenvalues spectrum. Then, the smallest Krylov space containing $x$ is $k$, the degree of the minimal polynomial of $A$. If the minimal polynomial has low degree then the Krylov space containing the solution is small, and a Krylov method converge fast. Eigenvalues play a central role to ensuring existence and uniqueness of Krylov solutions. \\

\paragraph{Definition:}{\it Given a nonsingular matrix $A \in \mathbb{C}^{m,m}$ and $y \ne 0 \in \mathbb{C}^{m}$ , the nth Krylov subspace $\mathcal{K}_{n}(A,y)$ generated by $A$ and $y$ is

$$
\mathcal{K}_{n}(A,y) = span\{y, Ay, \cdots , A^{n-1}y\}
$$
}

\subsection{Arnoldi iteration}

The problem of solving the system $Ax = b$ is closely related to sparse matrices eigenvalues research problem. We will briefly develop the eigenvalues problem (Arnoldi iteration), and then adapt it to our system $Ax = b$ problem. Arnoldi method finds the eigenvalues of general (possibly non-Hermitian) matrices; an analogous method for Hermitian matrices is the Lanczos iteration. The Arnoldi iteration was invented by W. E. Arnoldi in 1951.\\
The Arnoldi's method is based on the {\it power iteration algorithm} (iteration Von Mises), we will describe later. Indeed, the construction successive powers of a matrix A will give us the eigenvector $| \lambda_{1} >$ of the dominant eigenvalue $\lambda_{1}$. At first we will describe the Gram-Schmidt algorithm, which is the base line for all the following algorithm.

\paragraph{Gram-Schmidt algorithm}{ The Gram-Schmidt method allows, starting from a set of random vectors $\{v_{1}, v_{2}, \dots, v_{n}\}$, to construct an orthogonal basis $\{u_{1}, u_{2}, \dots, u_{n}\}$ or $e_{1}, e_{2}, \dots, e_{n}$ orthonormal.

$$
\begin{array}{rclcl}
u_{1} & = & v_{1} & \Leftrightarrow& e_{1} = u_{1} / \| u_{1} \| \\
u_{2} & = & v_{2} - P_{u_{1}}(v_{2})& \Leftrightarrow& e_{2} = u_{2} / \| u_{2} \| \\
u_{3} & = & v_{2} - P_{u_{1}}(v_{3}) - P_{u_{2}}(v_{3})& \Leftrightarrow& e_{3} = u_{3} / \| u_{3} \| \\
\vdots &&&& \\
u_{n} & = & v_{n} - \sum_{i = 1}^{n-1} P_{u_{i}}(v_{n})& \Leftrightarrow& e_{n} = u_{n} / \| u_{n} \| \\
\end{array}
$$

The idea is to orthogonalize the vector $v_{n}$ by removing its projections componant on the basis $e_{1}, e_{2}, \dots, e_{n-1}$.
}

\paragraph{Power iteration algorithm}{

L'algorithme de power iteration suit le même principe que l'algorithme Gram-Schmidt. L'algorithme commence avec un vecteur $| b_{0} >$ avec des composantes aléatoires. Le vecteur suivant est construit de la façon suitante:

$$
| b_{k+1} > = \frac{A | b_{k} >}{\|| b_{k} >\|}
$$

Nous atteignons la convergence quand $| b_{k} > = | b_{k+1} > / \| | b_{k+1} > \|$. En d'autres termes, si nous normailsons à chaque étape, nous avons:

$$
| e_{k} > = \frac{| b_{k} > }{ \| | b_{k} > \|}
$$

Alors:

$$
| b_{k+1} > = A | e_{k} > and | e_{k+1} > = \frac{| b_{k+1} > }{ \| | b_{k+1} > \|}
$$

Nous atteignons la convergence quand $| e_{k+1} > = | e_{k} > = | \lambda_{1} >$.  Ce qui veut dire:

$$
\underset{\lambda_{1}}{\| | b_{k+1} > \|}| e_{k+1} > = A| e_{k} > \Leftrightarrow \lambda_{1} | \lambda_{1} > = A| \lambda_{1} >
$$
}

L'algorithme de {\it Power iteration} est généralisé pour créer l'espace de Krylov.

%http://en.wikipedia.org/wiki/Arnoldi_iteration

$$
H_{n} = \left(
\begin{array}{ccccc}
h_{1,1} & h_{1,2} & h_{1,3} & \cdots & h_{1,n} \\
h_{2,1} & h_{2,2} & h_{2,3} & \cdots & h_{2,n} \\
0       & h_{3,2} & h_{3,3} & \cdots & h_{3,n} \\
\vdots  & \ddots  & \ddots  & \ddots & \vdots  \\
0       & \hdots  & 0       & h_{n,n-1} & h_{n,n} \\
\end{array}
\right)
$$

$$
\tilde{H}_{n} = \left(
\begin{array}{ccccc}
h_{1,1} & h_{1,2} & h_{1,3} & \cdots & h_{1,n} \\
h_{2,1} & h_{2,2} & h_{2,3} & \cdots & h_{2,n} \\
0       & h_{3,2} & h_{3,3} & \cdots & h_{3,n} \\
\vdots  & \ddots  & \ddots  & \ddots & \vdots  \\
0       & \hdots  & 0       & h_{n,n-1} & h_{n,n} \\
0       & \hdots  & \hdots  & 0         & h_{n+1,n} \\
\end{array}
\right)
$$

\subsection{GMRES}


\subsection{Preconditioners}



\newpage

%-------------------------------------------------------------------------------

\section{FEniCS}

\newpage

%-------------------------------------------------------------------------------

\section{To review}











Dans les applications de calcul, du type éléments finis, plus le maillage est dense plus la précision du calcul augmente. Cependant, la quantité de temps nécessaire à l'accomplissement d'un calcul basé sur un maillage est souvent proportionnelle au nombre d'éléments du maillage. Un compromis doit donc être trouvé entre la précision et le temps de calcul. 



La méthode des éléments finis permet la résolution approchée de systèmes d'équations aux dérivées partielles, en cherchant une solution approchée du problème qui s'exprime comme combinaison linéaire d'un ensemble de fonctions élémentaires. Le choix de l'ensemble des fonctions élémentaires est très important pour la convergence de cette méthode, et est en général guidé par un maillage du domaine dans lequel on cherche à résoudre le système d'équations. A chaque élément du maillage est associé un petit ensemble de fonctions élémentaires adapté au problème, et la solution approchée est exprimée comme combinaison linéaire des fonctions élémentaires de tous les éléments du maillage. Pour calculer la solution approchée, il suffit alors d'inverser un système d'équations algébriques linéaires.
